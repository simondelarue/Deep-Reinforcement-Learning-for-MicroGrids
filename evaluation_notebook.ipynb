{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Notebook Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <font color='red'> WARNING : </font>  </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> No matter which approach you've chosen, you need to re-install any custom packages you had to install to make your code work ! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Install your packages below: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Total-RD/pymgrid/\n",
    "## Other packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('data/building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('data/building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation for Rule Based Approaches </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using Rule Based Algorithms, here is what your submitted code should feature no matter what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "import json # Necessary to export your results\n",
    "## Other packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Implementation of the rules that generate control dictionaries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXAMPLE FOR A NAIVE RULE BASED STRATEGY\n",
    "\"\"\"\n",
    "def naive_rule_based_strategy(building):\n",
    "    building_data = building.get_updated_values()\n",
    "\n",
    "    total_building_cost = 0\n",
    "\n",
    "    while building.done == False:\n",
    "        load = building_data['load']\n",
    "        pv = building_data['pv']\n",
    "        capa_to_charge = building_data['capa_to_charge']\n",
    "        capa_to_dischare = building_data['capa_to_discharge']\n",
    "\n",
    "        p_disc = max(0, min(load-pv, capa_to_dischare, building.battery.p_discharge_max))\n",
    "        p_char = max(0, min(pv-load, capa_to_charge, building.battery.p_charge_max))\n",
    "\n",
    "        if load - pv >=  0:\n",
    "\n",
    "            control_dict = {'battery_charge': 0,\n",
    "                                'battery_discharge': p_disc,\n",
    "                                'grid_import': max(0, load-pv-p_disc),\n",
    "                                'grid_export':0,\n",
    "                                'pv_consummed': min(pv, load),\n",
    "                           }\n",
    "\n",
    "\n",
    "        if load - pv <  0:\n",
    "\n",
    "            control_dict = {'battery_charge': p_char,\n",
    "                                'battery_discharge': 0,\n",
    "                                'grid_import': 0,\n",
    "                                'grid_export': max(0,pv-load-p_char),#abs(min(load-pv,0)),\n",
    "                                'pv_consummed': min(pv, load+p_char),\n",
    "                           }\n",
    "\n",
    "        building_data = building.run(control_dict)\n",
    "        total_building_cost += building.get_cost()\n",
    "    \n",
    "    return total_building_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Run of the rules on the Test environment </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rule based methods have no \"training\" as such, this means Training CPU Time will always be 0 and only\n",
    "Test CPU Time will represent frugality\n",
    "\"\"\"\n",
    "\n",
    "eval_start = time.process_time()\n",
    "\n",
    "total_building_costs = []\n",
    "\n",
    "for building in buildings:\n",
    "\n",
    "    building.reset(testing = True)\n",
    "\n",
    "    total_building_cost = naive_rule_based_strategy(building)\n",
    "    total_building_costs.append(total_building_cost)\n",
    "\n",
    "eval_end = time.process_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost_building_1 = total_building_costs[0]\n",
    "total_cost_building_2 = total_building_costs[1]\n",
    "total_cost_building_3 = total_building_costs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frugality = eval_end - eval_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost_building_1,\n",
    "    \"building_2_performance\" : total_cost_building_2,\n",
    "    \"building_3_performance\" : total_cost_building_3,\n",
    "    \"frugality\" : frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = ## Enter Team name here\n",
    "\n",
    "with open(team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation for \"Simple\" Reinforcement Learning based approaches <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'> <b> Be careful, the rewards returned by the Gym environment are negative ! Don't forget to multiply by -1 the total reward as the Profitability you will need to submit needs to be positive ! </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "import json # Necessary to export your results\n",
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment\n",
    "import DiscreteEnvironment_modified as DiscreteEnvironment_modified # Imposed Discrete Environment\n",
    "import numpy as np\n",
    "\n",
    "## Other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Agent & Environment Setup before your training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_environments = [\n",
    "    DiscreteEnvironment_modified.Environment(env_config={'building':buildings[i]}) for i in range(3)\n",
    "]\n",
    "\"\"\"\n",
    "Agent, potential Q-Table & other necessary setup code here \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions for Q-learning algo**  \n",
    "We used the functions developped in the following repo : https://github.com/Total-RD/pymgrid/blob/master/notebooks/A%20Q-Learning%20Example%20with%20PymGrid.ipynb, and modified them when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_qtable(env, nb_action, building):\n",
    "    \"\"\"\n",
    "    Initialize the Q-table of the Q-learning.\n",
    "    The state is an intersection between net load possibilities (from -(pv production) to load),\n",
    "    and the state of charge of the battery (from soc_min and soc_max).\n",
    "\n",
    "    -- Input :\n",
    "        env : Enivronment object\n",
    "        nb_action : Number of action can be taken by each state during the Q-learning [Integer]\n",
    "\n",
    "    -- Ouput :\n",
    "        Q : A dict of state containing the value of weight of each action for each state\n",
    "            (Dict of Dict)\n",
    "    \"\"\"\n",
    "\n",
    "    state = []\n",
    "    Q = {}\n",
    "\n",
    "    # --- Defining state possibilities ---------------------------\n",
    "    for i in range(-int(env.parameters[\"PV_rated_power\"] - 1), int(env.parameters[\"load\"] + 2)):\n",
    "        for j in np.arange(round(env.battery.soc_min, 1), round(env.battery.soc_max + 0.1, 1), 0.1):\n",
    "            state.append((i, round(j, 1)))\n",
    "\n",
    "    # --- Initialize Q(s, a) at zero ----------------------------\n",
    "    for s in state:\n",
    "        Q[s] = {}\n",
    "        for a in range(nb_action):\n",
    "            if building==3:\n",
    "                if a==4:\n",
    "                    Q[s][a] = -50\n",
    "                elif a==6:\n",
    "                    Q[s][a] = -30\n",
    "                else:\n",
    "                    Q[s][a] = 0\n",
    "            else:\n",
    "                Q[s][a] = 0\n",
    "\n",
    "    return Q\n",
    "\n",
    "def epsilon_decreasing_greedy(action, epsilon, nb_action):\n",
    "    \"\"\"\n",
    "    Adding random aleas for the choice of actions\n",
    "\n",
    "    -- Input :\n",
    "        action : integer representing the action taken [Integer]\n",
    "        epsilon : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                  aleas is taken [Float]\n",
    "        nb_action : Number of action can be taken by each state during the Q-learning [Integer]\n",
    "\n",
    "    -- Output :\n",
    "        action : integer representing the action taken [Integer]\n",
    "        randomm : binary value to consider if a aleas has been taken for action choice [Integer]\n",
    "    \"\"\"\n",
    "    p = np.random.random()\n",
    "\n",
    "    if p < (1 - epsilon):\n",
    "        randomm = 0\n",
    "        return action, randomm\n",
    "\n",
    "    else:\n",
    "        randomm = 1\n",
    "        return np.random.choice(nb_action), randomm\n",
    "    \n",
    "def max_dict(d):\n",
    "    \"\"\"\n",
    "    Returning the tuple (action, val) maximizing the reward in the Q-table depending of a state.\n",
    "    Reward correspond to the amount paid to answer the consumption (count negatively)\n",
    "    => Maximizing a negative number = Minimizing its absolute value\n",
    "\n",
    "    -- Input :\n",
    "        d : dict of action and reward associate of a state [Dict]\n",
    "\n",
    "    -- Output :\n",
    "        max_key : action corresponding to the maximal reward [Integer]\n",
    "        max_value : value of the maximal reward [Integer]\n",
    "    \"\"\"\n",
    "    max_key = None\n",
    "    max_val = float(\"-inf\")\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "\n",
    "    return max_key, max_val\n",
    "\n",
    "def update_epsilon(epsilon):\n",
    "    \"\"\"\n",
    "    Update epsilon value (share of aleas in the choice of an action) to minimize it through iteration.\n",
    "\n",
    "    -- Input :\n",
    "        epsilon : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                  aleas is taken [Float]\n",
    "\n",
    "    -- Output :\n",
    "        epsilon (updated) : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                            aleas is taken [Float]\n",
    "    \"\"\"\n",
    "    epsilon = epsilon - epsilon * 0.02\n",
    "\n",
    "    if epsilon < 0.1:\n",
    "        epsilon = 0.1\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "def change_name_action(idx, building):\n",
    "    \"\"\"\n",
    "    Print function\n",
    "    \"\"\"\n",
    "    if building==3:\n",
    "        if idx == 0:\n",
    "            action_name = \"PV + Charge + Export\"\n",
    "        elif idx == 5:\n",
    "            action_name = \"PV + Discharge + Import\"\n",
    "        elif idx == 2:\n",
    "            action_name = \"Import\"\n",
    "        elif idx == 3:\n",
    "            action_name = \"Full Export\"\n",
    "        elif idx == 4:\n",
    "            action_name = \"Genset\"\n",
    "        elif idx == 1:\n",
    "            action_name = \"Export/Import\"\n",
    "        elif idx == 6:\n",
    "            action_name = \"Genset\"\n",
    "    else:\n",
    "        if idx == 0:\n",
    "            action_name = \"PV + Charge + Export\"\n",
    "        elif idx == 1:\n",
    "            action_name = \"PV + Discharge + Import\"\n",
    "        elif idx == 2:\n",
    "            action_name = \"Import\"\n",
    "        elif idx == 3:\n",
    "            action_name = \"Full Export\"\n",
    "        elif idx == 4:\n",
    "            action_name = \"Export/Import\"\n",
    "    \n",
    "    return action_name\n",
    "\n",
    "def print_welcome(idx):\n",
    "    \"\"\"\n",
    "    Print function\n",
    "    \"\"\"\n",
    "    if idx == 0:\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"|        WELCOME TO PYMGRID        |\")\n",
    "        print(\"------------------------------------\")\n",
    "    elif idx == 1:\n",
    "\n",
    "        print(\"t -     STATE  -  ACTION - COST\")\n",
    "        print(\"================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Q_Learning_DE(env, nb_action, building, horizon):\n",
    "\n",
    "    # --- Defining parameters ----------------------------------\n",
    "    Q = init_qtable(env.mg, nb_action, building)\n",
    "    nb_state = len(Q)\n",
    "    nb_episode = 15\n",
    "    alpha = 0.1    #  Learning rate\n",
    "    epsilon = 0.1  #  Aleas\n",
    "    gamma = 0.99\n",
    "    record_cost = []\n",
    "    t0 = time.time()\n",
    "    t = t0\n",
    "    print_training = \"Training Progressing .   \"\n",
    "    print_welcome(0)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for e in range(nb_episode + 1):\n",
    "\n",
    "        # --- Initialize episode variables --------------------------\n",
    "        episode_cost = 0\n",
    "        env.reset()\n",
    "        net_load = round(env.mg.load - env.mg.pv)\n",
    "        soc = round(env.mg.battery.soc, 1)\n",
    "        s = (net_load, soc)  # First state\n",
    "        a = max_dict(Q[s])[0]  # First action\n",
    "        a, randomm = epsilon_decreasing_greedy(a, epsilon, nb_action)  # Adding aleas in the first action\n",
    "\n",
    "        # --- Q-learning accros horizon ------------------------------\n",
    "        for i in range(horizon):\n",
    "\n",
    "            # Run action choosen precedently\n",
    "            status, reward, done, info = env.step(a)\n",
    "            \n",
    "            # Compute cost with the previous actions\n",
    "            r = reward\n",
    "            episode_cost = env.get_cost()\n",
    "\n",
    "            # Update variables depending on the precedent action\n",
    "            net_load = round(env.mg.load - env.mg.pv)\n",
    "            soc = round(env.mg.battery.soc, 1)\n",
    "            s_ = (net_load, soc)\n",
    "            a_ = max_dict(Q[s_])[0]\n",
    "\n",
    "            if i == horizon - 1:\n",
    "                Q[s][a] += alpha * (r - Q[s][a])\n",
    "\n",
    "            # Update reward depending on the action choosen\n",
    "            else:\n",
    "                old_Q = Q[s][a]  # Previous reward\n",
    "                target = (r + gamma * Q[s_][a_])  # Target = reward of the previous action + expectation of reward of the last action\n",
    "                td_error = target - Q[s][a]  # Difference of cost between two episode\n",
    "                Q[s][a] = (1 - alpha) * Q[s][a] + alpha * td_error  # Update weight in the Q-table with the reward of the last action\n",
    "            s, a = s_, a_\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def testing_Q_Learning_DE(env, Q, horizon, building):\n",
    "\n",
    "    # --- Initialize variables --------------------------\n",
    "    env.reset()\n",
    "    net_load = round(env.mg.load - env.mg.pv)\n",
    "    soc = round(env.mg.battery.soc, 1)\n",
    "    s = (net_load, soc)\n",
    "    a = max_dict(Q[s])[0]\n",
    "    total_cost = 0\n",
    "    print_welcome(1)\n",
    "\n",
    "    # --- Q-learning accros horizon ----------------------\n",
    "    for i in range(horizon):\n",
    "\n",
    "        # Run action choosen precedently\n",
    "        action_name = change_name_action(a, building)        \n",
    "        status, reward, done, info = env.step(a)\n",
    "        cost = - reward\n",
    "        total_cost = env.get_cost()\n",
    "\n",
    "        # Print function\n",
    "        #if i % 500 == 0 or i == horizon - 1:\n",
    "        #print(i, \" -\", (int(net_load), soc), action_name, round(total_cost, 1), \"â‚¬\")\n",
    "\n",
    "        #  Update variables depending on the last action\n",
    "        net_load = round(env.mg.load - env.mg.pv)\n",
    "        soc = round(env.mg.battery.soc, 1)\n",
    "\n",
    "        #  Defining the next state and action corresponding\n",
    "        s_ = (net_load, soc)\n",
    "        a_ = max_dict(Q[s_])[0]\n",
    "        s, a = s_, a_\n",
    "        \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "\"\"\"\n",
    "Training code\n",
    "\"\"\"\n",
    "Q0_DE = training_Q_Learning_DE(building_environments[0], 5, 1, 8757)\n",
    "Q1_DE = training_Q_Learning_DE(building_environments[1], 5, 2, 8757)\n",
    "Q2_DE = training_Q_Learning_DE(building_environments[2], 7, 3, 8757)\n",
    "\n",
    "train_end = time.process_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frugality = train_end - train_start\n",
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "building_environments[0].reset(testing=True)\n",
    "total_cost[0] = testing_Q_Learning_DE(building_environments[0], Q0_DE, 8757, 1)\n",
    "\n",
    "building_environments[1].reset(testing=True)\n",
    "total_cost[1] = testing_Q_Learning_DE(building_environments[1], Q1_DE, 8757, 2)\n",
    "\n",
    "building_environments[2].reset(testing=True)\n",
    "total_cost[2] = testing_Q_Learning_DE(building_environments[2], Q2_DE, 8757, 3)\n",
    "    \n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an example for a Random Agent \n",
    "\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "  mean profitability and mean frugality\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "    \n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.randint(building_env.action_space.n)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]+=reward\n",
    "\n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frugality = test_end - test_start\n",
    "print(test_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = 'team35'\n",
    "\n",
    "with open('results/' + team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation for Deep Reinforcement Learning based approaches <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'> <b> Be careful, the rewards returned by the Gym environment are negative ! Don't forget to multiply by -1 the total reward as the Profitability you will need to submit needs to be positive ! </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1) Import all used libraries and scripts here </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2) Agent & Environment Setup before your training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3) Training of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQL(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, nb_actions):\n",
    "        super(DeepQL, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.nb_actions = nb_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.nb_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        if torch.cuda.is_available():       \n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, nb_actions,\n",
    "                max_mem_size = 100000, eps_end = 0.01, eps_dec = 5e-4):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(nb_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.Q_eval = DeepQL(self.lr, nb_actions=nb_actions, input_dims=input_dims,\n",
    "                            fc1_dims=64, fc2_dims=64)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype= np.bool)\n",
    "    \n",
    "    def store_transition(self, state, action ,reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    def choose_action (self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = torch.FloatTensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem,self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = torch.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next,dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=128, nb_actions=5,\n",
    "             eps_end=0.01, input_dims=[10], lr= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.process_time()\n",
    "\n",
    "nb_episode = 15\n",
    "\n",
    "for idx, env in enumerate(building_environments):\n",
    "    print(f\"building {idx}\")\n",
    "    for i in range(nb_episode):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score -= reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.train()\n",
    "            observation = observation_\n",
    "        print(f\"- episode : {i} | score : {score}\")\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Test of the agent </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an example for a Random Agent \n",
    "\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "mean profitability and mean frugality\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "\n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]-=reward\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5) Store & Export Results in JSON format </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "team_name = 'team35'\n",
    "\n",
    "with open('results/' + team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> For Optimisation based approaches <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the README and the Getting Started Notebook, the evaluation depends heavily on how you formulate the problem, no template can be given for this familly of methods "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
